{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron but Vector based\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron(X,T,lr=0.1,w=None):\n",
    "    X = np.hstack((np.array(X),np.ones((len(X),1)))) #Homogeneous coordinates\n",
    "    if w is None:\n",
    "        w = X[np.random.randint(0,len(X))].copy()\n",
    "    done = False\n",
    "    while not done:\n",
    "        done = True\n",
    "        for i,x in enumerate(X):\n",
    "            if T[i] * np.dot(x,w)<=0: # check for obuse\n",
    "                w += lr* x * T[i]\n",
    "                done = False\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hyperplane2d(X,T,w):\n",
    "    X = np.array(X)\n",
    "    T = np.array(T)\n",
    "    plt.plot(X[T==1,0],X[T==1,1],'go')\n",
    "    plt.plot(X[T==-1,0],X[T==-1,1],'ro')\n",
    "    xlim = plt.gca().get_xlim()\n",
    "    slope = -w[0]/w[1]\n",
    "    bias = -w[-1]/w[1]\n",
    "    plt.plot(xlim,[xlim[0]*slope+bias,xlim[1]*slope+bias],'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OR problem\n",
    "X = [[1,1],[1,-1],[-1,1],[-1,-1]]\n",
    "T = [1,1,1,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AND problem\n",
    "X = [[1,1],[1,-1],[-1,1],[-1,-1]]\n",
    "T = [1,-1,-1,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XOR problem\n",
    "X = [[1,1],[1,-1],[-1,1],[-1,-1]]\n",
    "T = [-1,1,1,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = perceptron(X,T,lr=1e-1)\n",
    "print(w)\n",
    "plot_hyperplane2d(X,T,w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron but gradient based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_in =(2,) #init dimension (use 2d)\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(1,input_shape=d_in))\n",
    "model.add(tf.keras.layers.Activation('sigmoid'))\n",
    "model.compile(loss=tf.keras.losses.MeanSquaredError(),optimizer=tf.keras.optimizers.SGD(learning_rate=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#supervised learninng \n",
    "#มี x y\n",
    "#ปัญหา OR\n",
    "X = [[0,0],[0,1],[1,0],[1,1]]\n",
    "Y = [0,1,1,1]\n",
    "history = model.fit(X,Y,epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['loss'])\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = [[1,1],[1,0],[0,1],[0,0]]\n",
    "Z = model.predict(X_test)\n",
    "print(tf.round(Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Multi Layer Perceptron (MLP)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to slove XOR problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_in = (2,)\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(20,input_shape=d_in))\n",
    "model.add(tf.keras.layers.Activation('sigmoid'))\n",
    "model.add(tf.keras.layers.Dense(1))\n",
    "model.add(tf.keras.layers.Activation('sigmoid'))\n",
    "model.compile(loss=tf.keras.losses.MeanSquaredError(),\n",
    "                optimizer=tf.keras.optimizers.SGD(learning_rate=1.5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#XOR problem\n",
    "X = [[0,0],[0,1],[1,0],[1,1]]\n",
    "Y = [0,1,1,0]\n",
    "history = model.fit(X,Y,epochs=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['loss'])\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = [[1,1],[1,0],[0,1],[0,0]]\n",
    "Z = model.predict(X_test)\n",
    "print(tf.round(Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Slove Gradient Tape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "X = np.random.rand(N)\n",
    "Y = 5 * X +10 + 0.4 * np.random.rand(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.random.rand()\n",
    "b = np.random.rand()\n",
    "print(W,b)\n",
    "W = tf.Variable(W)  # weight\n",
    "b = tf.Variable(b)  # bias\n",
    "lr = 0.1 # learning rate\n",
    "for epoch in range(1000):\n",
    "    with tf.GradientTape() as t:\n",
    "        y = W * X + b\n",
    "        loss = tf.reduce_mean((y-Y)**2) # MSE\n",
    "    dW, db = t.gradient(loss,[W,b]) # de/dW , de/db\n",
    "    W.assign_sub(lr*dW) # W -= lr * dW\n",
    "    b.assign_sub(lr*db) # b -= lr * db\n",
    "    print(epoch,W.numpy(),b.numpy(),loss.numpy())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = W *X + b\n",
    "plt.plot(X,Y,'.') # actual\n",
    "plt.plot(X,Z,'.r') # predict\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Slove with keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.rand(40)\n",
    "y = 0.5 * x + 0.2 + 0.1 * np.random.rand(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_in = (1,)\n",
    "model = tf.keras.Sequential()\n",
    "# keras default bias = 1\n",
    "model.add(tf.keras.layers.Dense(1,input_shape=d_in)) # 1 = dimension of weight\n",
    "model.compile(loss=tf.keras.losses.MeanSquaredError(), # = > loss function\n",
    "                optimizer=tf.keras.optimizers.SGD(learning_rate=0.1))\n",
    "model.fit(x,y,epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = model.predict(x)\n",
    "plt.plot(x,y,'.')\n",
    "plt.plot(x,z[:,0],'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nonlinear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Slove with Gradient Tapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "X = np.random.rand(N,1)\n",
    "Y = np.sin(2*np.pi *X) + 0.4 * np.random.rand(N,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Nonlinear Regression we have to use multilayer perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer Architecture\n",
    "# input     l1      l2      output\n",
    "#   1   =>  100 =>  100 =>  1\n",
    "W1 = tf.Variable(np.random.randn(1,100)) # input : 1 , output : 100\n",
    "b1 = tf.Variable(np.random.randn(100))\n",
    "W2 = tf.Variable(np.random.randn(100,100)) # input : 100 , output : 100\n",
    "b2 = tf.Variable(np.random.randn(100))\n",
    "W3 = tf.Variable(np.random.randn(100,1)) # input : 100 , output : 1\n",
    "b3 = tf.Variable(np.random.randn(1))\n",
    " \n",
    "# activation function\n",
    "def relu(x):\n",
    "    return tf.where(x>=0,x,0) # if x >= 0: return x | else: return 0\n",
    "\n",
    "lr = 0.0001\n",
    "for epoch in range(7000):\n",
    "    with tf.GradientTape() as t:\n",
    "        y = relu(X @ W1 + b1)\n",
    "        y = relu(y @ W2 + b2)\n",
    "        y = y @ W3 + b3\n",
    "        loss = tf.reduce_mean((y-Y)**2) # MSE\n",
    "    dW1, db1, dW2, db2, dW3, db3 = t.gradient(loss,[W1,b1,W2,b2,W3,b3])\n",
    "    W1.assign_sub(lr * dW1)\n",
    "    b1.assign_sub(lr * db1)\n",
    "    W2.assign_sub(lr * dW2)\n",
    "    b2.assign_sub(lr * db2)\n",
    "    W3.assign_sub(lr * dW3)\n",
    "    b3.assign_sub(lr * db3)\n",
    "    if epoch % 1000 == 0 :\n",
    "        print(loss.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = relu(X @ W1 +b1)\n",
    "Z = relu(Z @ W2 +b2)\n",
    "Z = Z @ W3 + b3\n",
    "print(Z)\n",
    "plt.plot(X, Z,'.r')\n",
    "plt.plot(X,Y,'.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### slove with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.rand(200)\n",
    "y = 0.5 * x**4 + 0.2 + 0.1*np.random.rand(200)\n",
    "plt.plot(x, y, '.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_in = (1,)\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(200, input_shape=d_in, \n",
    "                                activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(200,\n",
    "                                activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1))\n",
    "model.compile(loss=tf.keras.losses.MeanSquaredError(),\n",
    "                        optimizer=tf.keras.optimizers.SGD(learning_rate=0.1))\n",
    "\n",
    "model.fit(x, y, epochs=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = model.predict(x)\n",
    "plt.plot(x, y, '.')\n",
    "plt.plot(x, z[:,0], '.r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Tapes with Class 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "X = np.random.rand(N,1)\n",
    "Y = np.sin(2*np.pi *X) + 0.4 * np.random.rand(N,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return tf.where(x>=0,x,0)\n",
    "\n",
    "class MLP():\n",
    "    def __init__(self,neurons=[1,100,100,1], activation=[relu,relu,None]):\n",
    "        self.W = []\n",
    "        self.activation = activation\n",
    "        for i in range(1,len(neurons)):\n",
    "            self.W.append(tf.Variable(np.random.randn(neurons[i-1],neurons[i]))) # weight\n",
    "            self.W.append(tf.Variable(np.random.randn(neurons[i]))) # bias\n",
    "    \n",
    "    def __call__(self,x):\n",
    "        for i in range(0,len(self.W),2):\n",
    "            x = x @ self.W[i] + self.W[i+1]\n",
    "            if self.activation[i // 2] is not None:\n",
    "                x = self.activation[i//2](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0001\n",
    "model = MLP()\n",
    "for epoch in range(7000):\n",
    "    with tf.GradientTape() as t:\n",
    "        loss =tf.reduce_mean((model(X)-Y)**2)\n",
    "    dW = t.gradient(loss,model.W)\n",
    "    for i, W in enumerate(model.W):\n",
    "        W.assign_sub(lr*dW[i])\n",
    "    if epoch % 1000 == 0:\n",
    "        print(loss.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = model(X)\n",
    "plt.plot(X, Y, '.')\n",
    "plt.plot(X, Z[:,0], '.r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Tapes with Class 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "X = np.random.rand(N,1)\n",
    "Y = np.sin(2*np.pi *X) + 0.4 * np.random.rand(N,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return tf.where(x>=0,x,0)\n",
    "\n",
    "class MLP():\n",
    "    def __init__(self,neurons = [1,100,100,1], activation = [relu,relu,None]):\n",
    "        self.W = []\n",
    "        self.activation = activation\n",
    "        for i in range(1,len(neurons)):\n",
    "            self.W.append(tf.Variable(np.random.randn(neurons[i-1],neurons[i]))) # weight\n",
    "            self.W.append(tf.Variable(np.random.randn(neurons[i]))) # bias\n",
    "    \n",
    "    def __call__(self,x):\n",
    "        for i in range(0,len(self.W),2):\n",
    "            x = x @ self.W[i] + self.W[i+1]\n",
    "            if self. activation[i//2] is not None:\n",
    "                x = self.activation[i//2](x)\n",
    "        return X\n",
    "    \n",
    "    def fit(self,X,Y, lr=0.0001, epochs=2000):\n",
    "        for epoch in range(epochs):\n",
    "            with tf.GradientTape() as t:\n",
    "                loss =tf.reduce_mean((self(X)-Y)**2)\n",
    "            print('loss',loss)\n",
    "            dW = t.gradient(loss,self.W)\n",
    "            print('dW',dW)\n",
    "            for i,W in enumerate(self.W):\n",
    "                W.assign_sub(lr * dW[i])\n",
    "            if epoch % 1000 == 0:\n",
    "                print(epoch,loss.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss tf.Tensor(0.9930106819515373, shape=(), dtype=float64)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'float' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-77fa9506b4fd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0001\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMLP\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m7000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-17-280d868d69e5>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, Y, lr, epochs)\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[0mdW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mW\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m                 \u001b[0mW\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massign_sub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mdW\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m1000\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'float' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "lr = 0.0001\n",
    "model = MLP()\n",
    "model.fit(X,Y,lr,7000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = model(X)\n",
    "plt.plot(X, Y, '.')\n",
    "plt.plot(X, Z[:,0], '.r')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8fac594bfae6525c0c41b4041d2d72effa188cc8ead05f81b1fab2bb098927fb"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
